{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1VW6uXAaXok_i2_jEJE5vaNdyiGYVZhCp","authorship_tag":"ABX9TyMC9x8YGL5d0qHDu4+8qEs8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# 실제 train data를 변환하기 - 데이터 읽어들이기\n","\n","import json\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","base_dir = os.getcwd()\n","\n","train_images_path = base_dir + '/data/train/images/'\n","train_labels_json_path = base_dir + '/data/train/labels_json/'\n","train_labels_path = base_dir + '/data/train/labels/'\n","\n","os.makedirs(train_images_path, exist_ok=True)\n","os.makedirs(train_labels_json_path, exist_ok=True)\n","os.makedirs(train_labels_path, exist_ok=True)\n","\n","!unzip -qq '/content/drive/MyDrive/KCC3기_2차_프로젝트/train.zip' -d '/content/data/train'\n","\n","train_images_list = os.listdir(train_images_path)\n","train_labels_json_list = os.listdir(train_labels_json_path)\n","\n","train_images_list.sort()\n","train_labels_json_list.sort()"],"metadata":{"id":"Rph6LxQM2xd1","executionInfo":{"status":"ok","timestamp":1692322351614,"user_tz":-540,"elapsed":207227,"user":{"displayName":"정과철","userId":"05060673751360488451"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# 실제 train data를 변환하기 - 라벨링 변환\n","\n","yolo_class = {\n","    '철캔': 0,\n","    '알루미늄캔': 4,\n","    '종이': 8,\n","    '무색단일': 12,\n","    '유색단일': 16,\n","    'PE': 20,\n","    'PP': 24,\n","    'PS': 28,\n","    '스티로폼': 32,\n","    '비닐': 36,\n","    '갈색': 40,\n","    '녹색': 44,\n","    '투명': 48\n","}\n","\n","for label in train_labels_json_list:\n","  file_json = open(train_labels_json_path + label)\n","  f_json = json.load(file_json)\n","\n","  img_width = f_json['IMAGE_INFO']['IMAGE_WIDTH']\n","  img_height = f_json['IMAGE_INFO']['IMAGE_HEIGHT']\n","\n","  f_txt = open(train_labels_path + label.replace('.json', '.txt'), 'w')\n","\n","  for anno in f_json['ANNOTATION_INFO']:\n","    obj_id = yolo_class[anno['DETAILS']]\n","\n","    if anno['DAMAGE'] != '원형':\n","      obj_id += 2\n","    if anno['DIRTINESS'] != '오염없음':\n","      obj_id += 1\n","\n","    obj_points = np.array(anno['POINTS'])\n","    if obj_points[0].size == 4:\n","      yolo_id = obj_id\n","      yolo_points = np.array([obj_points[0][0]+obj_points[0][2]/2, obj_points[0][1]+obj_points[0][3]/2,\n","                              obj_points[0][2], obj_points[0][3]])\n","\n","      yolo_points_norm = np.array([yolo_points[0]/img_width, yolo_points[1]/img_height,\n","                             yolo_points[2]/img_width, yolo_points[3]/img_height])\n","\n","      yolo_points_norm = list(yolo_points_norm)\n","      f_txt.write(str(yolo_id) + ' ')\n","      f_txt.write(' '.join(str(ele) for ele in yolo_points_norm))\n","      f_txt.write('\\n')\n","    elif obj_points.size >= 2:\n","      max_x = 0\n","      min_x = img_width\n","      max_y = 0\n","      min_y = img_height\n","      for points in obj_points:\n","        if points[0] > max_x:\n","          max_x = points[0]\n","        if points[0] < min_x:\n","          min_x = points[0]\n","        if points[1] > max_y:\n","          max_y = points[1]\n","        if points[1] < min_y:\n","          min_y = points[1]\n","\n","      yolo_id = obj_id\n","      yolo_points = np.array([(min_x+max_x)/2, (min_y+max_y)/2,\n","                              max_x-min_x, max_y-min_y])\n","      yolo_points_norm = np.array([yolo_points[0]/img_width, yolo_points[1]/img_height,\n","                             yolo_points[2]/img_width, yolo_points[3]/img_height])\n","      yolo_points_norm = list(yolo_points_norm)\n","\n","      f_txt.write(str(yolo_id) + ' ')\n","      f_txt.write(' '.join(str(ele) for ele in yolo_points_norm))\n","      f_txt.write('\\n')\n","    else:\n","      print(label)\n","  f_txt.close()\n"],"metadata":{"id":"VRsmJUw6P9QF","executionInfo":{"status":"ok","timestamp":1692322361512,"user_tz":-540,"elapsed":5743,"user":{"displayName":"정과철","userId":"05060673751360488451"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# json 라벨 삭제 및 라벨링 확인\n","\n","import shutil\n","\n","shutil.rmtree('/content/data/train/labels_json')\n","\n","print(open('/content/data/train/labels/606511@4_04002_220907_P1_T1.txt').readline())\n","print(open('/content/data/train/labels/10349@3_01001_220715_P1_T1.txt').readline())"],"metadata":{"id":"umdDdbHdvrkl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692322366568,"user_tz":-540,"elapsed":1125,"user":{"displayName":"정과철","userId":"05060673751360488451"}},"outputId":"01298850-6e80-4415-fdb2-f2558c882573"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["24 0.4915269230769231 0.49103166345617255 0.38315604395604386 0.6721641025641029\n","\n","0 0.6400563522733375 0.3877540579713878 0.1944409271364939 0.387188312642922\n","\n"]}]},{"cell_type":"code","source":["# validation 데이터 분리\n","\n","import shutil\n","\n","val_images_path = base_dir + '/data/val/images/'\n","val_labels_path = base_dir + '/data/val/labels/'\n","\n","os.makedirs(val_images_path, exist_ok=True)\n","os.makedirs(val_labels_path, exist_ok=True)\n","\n","train_images_list = os.listdir(train_images_path)\n","train_labels_list = os.listdir(train_labels_path)\n","\n","train_images_list.sort()\n","train_labels_list.sort()\n","\n","for i in range(2600):\n","  shutil.move(train_images_path + train_images_list[i*10], val_images_path)\n","  shutil.move(train_labels_path + train_labels_list[i*10], val_labels_path)\n"],"metadata":{"id":"VssZiUbMsxlq","executionInfo":{"status":"ok","timestamp":1692322376400,"user_tz":-540,"elapsed":875,"user":{"displayName":"정과철","userId":"05060673751360488451"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 분리한 데이터 이미지와 라벨 파일 일치 여부 확인\n","\n","print(len(os.listdir('/content/data/train/images/')))\n","print(len(os.listdir('/content/data/train/labels/')))\n","\n","print(len(os.listdir('/content/data/val/images/')))\n","print(len(os.listdir('/content/data/val/labels/')))\n","\n","train_images_list = os.listdir(train_images_path)\n","train_labels_list = os.listdir(train_labels_path)\n","\n","train_images_list.sort()\n","train_labels_list.sort()\n","\n","for i in range(len(train_images_list)):\n","  if train_images_list[i].replace('.jpg', '') != train_labels_list[i].replace('.txt', ''):\n","    print(train_images_list[i])\n","\n","val_images_list = os.listdir(val_images_path)\n","val_labels_list = os.listdir(val_labels_path)\n","\n","val_images_list.sort()\n","val_labels_list.sort()\n","\n","for i in range(len(val_images_list)):\n","  if val_images_list[i].replace('.jpg', '') != val_labels_list[i].replace('.txt', ''):\n","    print(val_images_list[i])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d5ZFZP8ivw4f","executionInfo":{"status":"ok","timestamp":1692322382202,"user_tz":-540,"elapsed":481,"user":{"displayName":"정과철","userId":"05060673751360488451"}},"outputId":"b5e7516b-572d-4c26-9a33-c1dcde15cb5e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["23400\n","23400\n","2600\n","2600\n"]}]},{"cell_type":"code","source":["# YOLO 학습을 위한 yaml 파일 생성\n","\n","!pip install PyYAML\n","\n","import yaml\n","\n","data = {\n","    'train': '/content/data/train/images/',\n","    'val': '/content/data/val/images',\n","    'names': ['IronCan_NoDamNoDir', 'IronCan_NoDamDir', 'IronCan_DamNoDir', 'IronCan_DamDir',\n","              'AluminumCan_NoDamNoDir', 'AluminumCan_NoDamDir', 'AluminumCan_DamNoDir', 'AluminumCan_DamDir',\n","              'Paper_NoDamNoDir', 'Paper_NoDamDir', 'Paper_DamNoDir', 'Paper_DamDir',\n","              'PetNoColor_NoDamNoDir', 'PetNoColor_NoDamDir', 'PetNoColor_DamNoDir', 'PetNoColor_DamDir',\n","              'PetColor_NoDamNoDir', 'PetColor_NoDamDir', 'PetColor_DamNoDir', 'PetColor_DamDir',\n","              'PE_NoDamNoDir', 'PE_NoDamDir', 'PE_DamNoDir', 'PE_DamDir',\n","              'PP_NoDamNoDir', 'PP_NoDamDir', 'PP_DamNoDir', 'PP_DamDir',\n","              'PS_NoDamNoDir', 'PS_NoDamDir', 'PS_DamNoDir', 'PS_DamDir',\n","              'Styrofoam_NoDamNoDir', 'Styrofoam_NoDamDir', 'Styrofoam_DamNoDir', 'Styrofoam_DamDir',\n","              'Vinyl_NoDamNoDir', 'Vinyl_NoDamDir', 'Vinyl_DamNoDir', 'Vinyl_DamDir',\n","              'GlassBrown_NoDamNoDir', 'GlassBrown_NoDamDir', 'GlassBrown_DamNoDir', 'GlassBrown_DamDir',\n","              'GlassGreen_NoDamNoDir', 'GlassGreen_NoDamDir', 'GlassGreen_DamNoDir', 'GlassGreen_DamDir',\n","              'GlassTransparency_NoDamNoDir', 'GlassTransparency_NoDamDir', 'GlassTransparency_DamNoDir', 'GlassTransparency_DamDir'],\n","    'nc': 52\n","}\n","\n","yaml_name = 'recycle_data.yaml'\n","with open('/content/data/' + 'recycle_data.yaml', 'w') as f:\n","  yaml.dump(data, f)\n","\n","with open('/content/data/' + 'recycle_data.yaml', 'r') as f:\n","  recycle_yaml = yaml.safe_load(f)\n","  display(recycle_yaml)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"deGHfflNZfI6","executionInfo":{"status":"ok","timestamp":1692322393761,"user_tz":-540,"elapsed":5269,"user":{"displayName":"정과철","userId":"05060673751360488451"}},"outputId":"492742bc-f5d0-4203-c601-8ce51ef1fd94"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (6.0.1)\n"]},{"output_type":"display_data","data":{"text/plain":["{'names': ['IronCan_NoDamNoDir',\n","  'IronCan_NoDamDir',\n","  'IronCan_DamNoDir',\n","  'IronCan_DamDir',\n","  'AluminumCan_NoDamNoDir',\n","  'AluminumCan_NoDamDir',\n","  'AluminumCan_DamNoDir',\n","  'AluminumCan_DamDir',\n","  'Paper_NoDamNoDir',\n","  'Paper_NoDamDir',\n","  'Paper_DamNoDir',\n","  'Paper_DamDir',\n","  'PetNoColor_NoDamNoDir',\n","  'PetNoColor_NoDamDir',\n","  'PetNoColor_DamNoDir',\n","  'PetNoColor_DamDir',\n","  'PetColor_NoDamNoDir',\n","  'PetColor_NoDamDir',\n","  'PetColor_DamNoDir',\n","  'PetColor_DamDir',\n","  'PE_NoDamNoDir',\n","  'PE_NoDamDir',\n","  'PE_DamNoDir',\n","  'PE_DamDir',\n","  'PP_NoDamNoDir',\n","  'PP_NoDamDir',\n","  'PP_DamNoDir',\n","  'PP_DamDir',\n","  'PS_NoDamNoDir',\n","  'PS_NoDamDir',\n","  'PS_DamNoDir',\n","  'PS_DamDir',\n","  'Styrofoam_NoDamNoDir',\n","  'Styrofoam_NoDamDir',\n","  'Styrofoam_DamNoDir',\n","  'Styrofoam_DamDir',\n","  'Vinyl_NoDamNoDir',\n","  'Vinyl_NoDamDir',\n","  'Vinyl_DamNoDir',\n","  'Vinyl_DamDir',\n","  'GlassBrown_NoDamNoDir',\n","  'GlassBrown_NoDamDir',\n","  'GlassBrown_DamNoDir',\n","  'GlassBrown_DamDir',\n","  'GlassGreen_NoDamNoDir',\n","  'GlassGreen_NoDamDir',\n","  'GlassGreen_DamNoDir',\n","  'GlassGreen_DamDir',\n","  'GlassTransparency_NoDamNoDir',\n","  'GlassTransparency_NoDamDir',\n","  'GlassTransparency_DamNoDir',\n","  'GlassTransparency_DamDir'],\n"," 'nc': 52,\n"," 'train': '/content/data/train/images/',\n"," 'val': '/content/data/val/images'}"]},"metadata":{}}]},{"cell_type":"code","source":["# YOLO 학습 위한 환경\n","\n","%pip install ultralytics\n","\n","import ultralytics\n","ultralytics.checks()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pj_7qSp7X-Qk","executionInfo":{"status":"ok","timestamp":1692322419775,"user_tz":-540,"elapsed":19350,"user":{"displayName":"정과철","userId":"05060673751360488451"}},"outputId":"04d29176-f47e-4a69-b975-f940c77b9228"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Ultralytics YOLOv8.0.157 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","Setup complete ✅ (2 CPUs, 12.7 GB RAM, 36.5/78.2 GB disk)\n"]}]},{"cell_type":"code","source":["# YOLO 전이 학습\n","from ultralytics import YOLO\n","import shutil, os\n","\n","model = YOLO('yolov8n.pt')\n","\n","model.train(data='/content/data/recycle_data.yaml', epochs=5, patience=3, batch=64, imgsz=640, name='recycle', exist_ok=True)\n","\n","if os.path.isdir('/content/drive/MyDrive/KCC_2nd_project/models/recycle'):\n","  shutil.rmtree('/content/drive/MyDrive/KCC_2nd_project/models/recycle')\n","\n","shutil.copytree('/content/runs/detect/recycle/' , '/content/drive/MyDrive/KCC_2nd_project/models/recycle/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1lRaMAmgeD4A","executionInfo":{"status":"ok","timestamp":1692326297970,"user_tz":-540,"elapsed":3718650,"user":{"displayName":"정과철","userId":"05060673751360488451"}},"outputId":"a418a481-9e9a-4b1d-8ca8-eec1d2451bba"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n","100%|██████████| 6.23M/6.23M [00:00<00:00, 106MB/s]\n","Ultralytics YOLOv8.0.157 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/data/recycle_data.yaml, epochs=5, patience=3, batch=64, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=recycle, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/recycle\n","Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","100%|██████████| 755k/755k [00:00<00:00, 23.6MB/s]\n","Overriding model.yaml nc=80 with nc=52\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n"," 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n"," 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"," 22        [15, 18, 21]  1    761452  ultralytics.nn.modules.head.Detect           [52, [64, 128, 256]]          \n","Model summary: 225 layers, 3020988 parameters, 3020972 gradients\n","\n","Transferred 319/355 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/recycle', view at http://localhost:6006/\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data/train/labels... 23400 images, 0 backgrounds, 0 corrupt: 100%|██████████| 23400/23400 [01:00<00:00, 385.91it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data/train/labels.cache\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data/val/labels... 2600 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2600/2600 [00:08<00:00, 300.27it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/data/val/labels.cache\n","Plotting labels to runs/detect/recycle/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000179, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/detect/recycle\u001b[0m\n","Starting training for 5 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","        1/5       9.4G      0.515      3.814       1.08         88        640: 100%|██████████| 366/366 [11:05<00:00,  1.82s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:45<00:00,  2.15s/it]\n","                   all       2600       2600      0.582      0.232      0.164      0.156\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","        2/5      8.61G      0.454      2.349      1.032        102        640: 100%|██████████| 366/366 [10:50<00:00,  1.78s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:42<00:00,  2.04s/it]\n","                   all       2600       2600      0.622      0.373      0.321      0.307\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","        3/5      8.61G     0.4156      1.757     0.9948         79        640: 100%|██████████| 366/366 [10:56<00:00,  1.80s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:45<00:00,  2.18s/it]\n","                   all       2600       2600       0.64      0.409      0.393      0.378\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","        4/5      8.61G     0.3887      1.466     0.9731         86        640: 100%|██████████| 366/366 [11:06<00:00,  1.82s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:43<00:00,  2.09s/it]\n","                   all       2600       2600      0.573      0.476       0.44      0.426\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","        5/5      8.61G     0.3764      1.302     0.9663         84        640: 100%|██████████| 366/366 [10:59<00:00,  1.80s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:58<00:00,  2.76s/it]\n","                   all       2600       2600      0.528      0.548      0.493      0.478\n","\n","5 epochs completed in 0.986 hours.\n","Optimizer stripped from runs/detect/recycle/weights/last.pt, 6.3MB\n","Optimizer stripped from runs/detect/recycle/weights/best.pt, 6.3MB\n","\n","Validating runs/detect/recycle/weights/best.pt...\n","Ultralytics YOLOv8.0.157 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","Model summary (fused): 168 layers, 3015788 parameters, 0 gradients\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:51<00:00,  2.47s/it]\n","                   all       2600       2600      0.528      0.548      0.492      0.478\n","    IronCan_NoDamNoDir       2600        103      0.375      0.971      0.684      0.592\n","      IronCan_NoDamDir       2600         69      0.336      0.768      0.439      0.433\n","      IronCan_DamNoDir       2600         21      0.343      0.286      0.277      0.276\n","        IronCan_DamDir       2600         12      0.223      0.333      0.316      0.316\n","AluminumCan_NoDamNoDir       2600         48      0.438      0.958      0.825      0.714\n","  AluminumCan_NoDamDir       2600          3          0          0     0.0424     0.0332\n","  AluminumCan_DamNoDir       2600        125       0.54      0.976      0.856      0.849\n","    AluminumCan_DamDir       2600         23      0.185      0.565      0.184      0.167\n","      Paper_NoDamNoDir       2600         64      0.576      0.828      0.771      0.756\n","        Paper_NoDamDir       2600          6          0          0      0.135      0.132\n","        Paper_DamNoDir       2600        116      0.612       0.94      0.778      0.764\n","          Paper_DamDir       2600         14      0.213     0.0714      0.169      0.167\n"," PetNoColor_NoDamNoDir       2600         32      0.717      0.875      0.852      0.828\n","   PetNoColor_NoDamDir       2600        109      0.657      0.972       0.91      0.899\n","   PetNoColor_DamNoDir       2600         26      0.538          1      0.923      0.902\n","     PetNoColor_DamDir       2600         33      0.357      0.824       0.67      0.657\n","   PetColor_NoDamNoDir       2600         31      0.273      0.509      0.326      0.326\n","     PetColor_NoDamDir       2600        148      0.362      0.932      0.602      0.441\n","     PetColor_DamNoDir       2600         10      0.484      0.378      0.423      0.423\n","       PetColor_DamDir       2600         14       0.26      0.571      0.485      0.484\n","         PE_NoDamNoDir       2600         23      0.234      0.217      0.195      0.183\n","           PE_NoDamDir       2600        169      0.485      0.994      0.855      0.848\n","           PE_DamNoDir       2600          2          1          0      0.248      0.248\n","             PE_DamDir       2600          5      0.568      0.282      0.668      0.668\n","         PP_NoDamNoDir       2600        108      0.765      0.903      0.878      0.867\n","           PP_NoDamDir       2600         75      0.316       0.88      0.688       0.65\n","           PP_DamNoDir       2600          9          1          0     0.0883     0.0883\n","             PP_DamDir       2600          6          1          0       0.23      0.223\n","         PS_NoDamNoDir       2600         83      0.646      0.843       0.85       0.84\n","           PS_NoDamDir       2600        102      0.513      0.922       0.81      0.804\n","           PS_DamNoDir       2600          9      0.536      0.222      0.461      0.461\n","             PS_DamDir       2600          5          1          0     0.0277     0.0277\n","  Styrofoam_NoDamNoDir       2600        159      0.765      0.987      0.916      0.916\n","    Styrofoam_NoDamDir       2600         13      0.141      0.593      0.175      0.175\n","    Styrofoam_DamNoDir       2600         25      0.403      0.136       0.35       0.35\n","      Styrofoam_DamDir       2600          1          1          0    0.00642    0.00642\n","      Vinyl_NoDamNoDir       2600         58      0.322      0.621      0.448      0.433\n","        Vinyl_NoDamDir       2600         10     0.0951        0.5      0.188      0.188\n","        Vinyl_DamNoDir       2600        124      0.468       0.96      0.739       0.72\n","          Vinyl_DamDir       2600          8     0.0962      0.125     0.0572     0.0558\n"," GlassBrown_NoDamNoDir       2600          1          1          0          0          0\n","   GlassBrown_NoDamDir       2600        199      0.967       0.98      0.987      0.971\n"," GlassGreen_NoDamNoDir       2600          1          1          0     0.0153     0.0153\n","   GlassGreen_NoDamDir       2600        193      0.819      0.964      0.964      0.949\n","     GlassGreen_DamDir       2600          1          1          0          0          0\n","GlassTransparency_NoDamNoDir       2600         10      0.483        0.9      0.674      0.669\n","GlassTransparency_NoDamDir       2600        194      0.683      0.985      0.959      0.948\n","Speed: 1.1ms preprocess, 2.7ms inference, 0.0ms loss, 1.9ms postprocess per image\n","Results saved to \u001b[1mruns/detect/recycle\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/KCC_2nd_project/models/recycle/'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# google drive의 모델 파일을 불러들여 다시 전이 학습\n","# 학습 완료 시 다시 google drive로 모델 파일 이동\n","\n","from ultralytics import YOLO\n","import shutil, os\n","\n","model = YOLO('/content/drive/MyDrive/KCC_2nd_project/models/recycle5/weights/best.pt')\n","\n","model.train(data='/content/data/recycle_data.yaml', epochs=5, patience=5, batch=64, imgsz=640, name='recycle', exist_ok=True)\n","\n","model_num = 6\n","model_dir = 'recycle' + str(model_num) + '/'\n","\n","if os.path.isdir('/content/drive/MyDrive/KCC_2nd_project/models/' + model_dir):\n","  shutil.rmtree('/content/drive/MyDrive/KCC_2nd_project/models/' + model_dir)\n","\n","shutil.copytree('/content/runs/detect/recycle/' , '/content/drive/MyDrive/KCC_2nd_project/models/' + model_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wKeyLkkA8vk","outputId":"dbb4b87c-d8db-459e-bfae-12a850db3dc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Ultralytics YOLOv8.0.157 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=/content/drive/MyDrive/KCC_2nd_project/models/recycle5/weights/best.pt, data=/content/data/recycle_data.yaml, epochs=5, patience=5, batch=64, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=recycle, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/recycle\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n"," 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n"," 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"," 22        [15, 18, 21]  1    761452  ultralytics.nn.modules.head.Detect           [52, [64, 128, 256]]          \n","Model summary: 225 layers, 3020988 parameters, 3020972 gradients\n","\n","Transferred 355/355 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/recycle', view at http://localhost:6006/\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data/train/labels.cache... 23400 images, 0 backgrounds, 0 corrupt: 100%|██████████| 23400/23400 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data/val/labels.cache... 2600 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2600/2600 [00:00<?, ?it/s]\n","Plotting labels to runs/detect/recycle/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000179, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/detect/recycle\u001b[0m\n","Starting training for 5 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","        1/5      9.31G     0.3384     0.7265     0.9406         88        640: 100%|██████████| 366/366 [10:35<00:00,  1.74s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:44<00:00,  2.14s/it]\n","                   all       2600       2600        0.7       0.68      0.712      0.704\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","        2/5      8.59G     0.3206     0.6085     0.9237        102        640: 100%|██████████| 366/366 [10:57<00:00,  1.80s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:42<00:00,  2.02s/it]\n","                   all       2600       2600      0.617      0.707      0.719       0.71\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","        3/5      8.59G     0.3158     0.5779     0.9213         79        640: 100%|██████████| 366/366 [10:50<00:00,  1.78s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:48<00:00,  2.30s/it]\n","                   all       2600       2600      0.652      0.687      0.713      0.704\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","        4/5      8.59G     0.3084     0.5637     0.9175         86        640: 100%|██████████| 366/366 [10:43<00:00,  1.76s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 21/21 [00:44<00:00,  2.10s/it]\n","                   all       2600       2600      0.646      0.727      0.732      0.723\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","        5/5      8.59G     0.3129     0.5706     0.9233        151        640:  46%|████▌     | 168/366 [04:55<04:40,  1.42s/it]"]}]},{"cell_type":"code","source":["# 예측 값 확인\n","\n","from ultralytics import YOLO\n","import shutil, os\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","model = YOLO('/content/drive/MyDrive/KCC_2nd_project/models/recycle3/weights/best.pt')\n","\n","\n","for i in range(10):\n","  results = model('/content/data/val/images/' + os.listdir('/content/data/val/images/')[i])\n","\n","  # Show the results\n","  for r in results:\n","    im_array = r.plot()  # plot a BGR numpy array of predictions\n","    im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n","    plt.imshow(im)\n","    plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1bYa7WdJzj5Wjvn0DlClNVBj5DB_2wTw2"},"id":"y-1X0xq4hsHK","executionInfo":{"status":"ok","timestamp":1692334587780,"user_tz":-540,"elapsed":6947,"user":{"displayName":"정과철","userId":"05060673751360488451"}},"outputId":"5618ec42-af27-4143-f405-d17a486ad201"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}